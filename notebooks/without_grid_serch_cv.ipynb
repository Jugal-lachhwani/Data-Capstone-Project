{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94935b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import scipy.sparse\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========================== CONFIGURATION ==========================\n",
    "CONFIG = {\n",
    "    \"data_path\": \"notebooks/data.csv\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"mlflow_tracking_uri\": \"https://dagshub.com/vikashdas770/YT-Capstone-Project.mlflow\",\n",
    "    \"dagshub_repo_owner\": \"vikashdas770\",\n",
    "    \"dagshub_repo_name\": \"YT-Capstone-Project\",\n",
    "    \"experiment_name\": \"Bow vs TfIdf\"\n",
    "}\n",
    "\n",
    "# ========================== SETUP MLflow & DAGSHUB ==========================\n",
    "mlflow.set_tracking_uri(CONFIG[\"mlflow_tracking_uri\"])\n",
    "dagshub.init(repo_owner=CONFIG[\"dagshub_repo_owner\"], repo_name=CONFIG[\"dagshub_repo_name\"], mlflow=True)\n",
    "mlflow.set_experiment(CONFIG[\"experiment_name\"])\n",
    "\n",
    "# ========================== TEXT PREPROCESSING ==========================\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def removing_numbers(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    return re.sub(f\"[{re.escape(string.punctuation)}]\", ' ', text)\n",
    "\n",
    "def removing_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def normalize_text(df):\n",
    "    try:\n",
    "        df['review'] = df['review'].apply(lower_case)\n",
    "        df['review'] = df['review'].apply(remove_stop_words)\n",
    "        df['review'] = df['review'].apply(removing_numbers)\n",
    "        df['review'] = df['review'].apply(removing_punctuations)\n",
    "        df['review'] = df['review'].apply(removing_urls)\n",
    "        df['review'] = df['review'].apply(lemmatization)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error during text normalization: {e}\")\n",
    "        raise\n",
    "\n",
    "# ========================== LOAD & PREPROCESS DATA ==========================\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = normalize_text(df)\n",
    "        df = df[df['sentiment'].isin(['positive', 'negative'])]\n",
    "        df['sentiment'] = df['sentiment'].replace({'negative': 0, 'positive': 1}).infer_objects(copy=False)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# ========================== FEATURE ENGINEERING ==========================\n",
    "VECTORIZERS = {\n",
    "    'BoW': CountVectorizer(),\n",
    "    'TF-IDF': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "ALGORITHMS = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# ========================== TRAIN & EVALUATE MODELS ==========================\n",
    "def train_and_evaluate(df):\n",
    "    with mlflow.start_run(run_name=\"All Experiments\") as parent_run:\n",
    "        for algo_name, algorithm in ALGORITHMS.items():\n",
    "            for vec_name, vectorizer in VECTORIZERS.items():\n",
    "                with mlflow.start_run(run_name=f\"{algo_name} with {vec_name}\", nested=True) as child_run:\n",
    "                    try:\n",
    "                        # Feature extraction\n",
    "                        X = vectorizer.fit_transform(df['review'])\n",
    "                        y = df['sentiment']\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=CONFIG[\"test_size\"], random_state=42)\n",
    "\n",
    "                        # Log preprocessing parameters\n",
    "                        mlflow.log_params({\n",
    "                            \"vectorizer\": vec_name,\n",
    "                            \"algorithm\": algo_name,\n",
    "                            \"test_size\": CONFIG[\"test_size\"]\n",
    "                        })\n",
    "\n",
    "                        # Train model\n",
    "                        model = algorithm\n",
    "                        model.fit(X_train, y_train)\n",
    "\n",
    "                        # Log model parameters\n",
    "                        log_model_params(algo_name, model)\n",
    "\n",
    "                        # Evaluate model\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        metrics = {\n",
    "                            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                            \"precision\": precision_score(y_test, y_pred),\n",
    "                            \"recall\": recall_score(y_test, y_pred),\n",
    "                            \"f1_score\": f1_score(y_test, y_pred)\n",
    "                        }\n",
    "                        mlflow.log_metrics(metrics)\n",
    "\n",
    "                        # Log model\n",
    "                        # mlflow.sklearn.log_model(model, \"model\")\n",
    "                        input_example = X_test[:5] if not scipy.sparse.issparse(X_test) else X_test[:5].toarray()\n",
    "                        mlflow.sklearn.log_model(model, \"model\", input_example=input_example)\n",
    "\n",
    "                        # Print results for verification\n",
    "                        print(f\"\\nAlgorithm: {algo_name}, Vectorizer: {vec_name}\")\n",
    "                        print(f\"Metrics: {metrics}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in training {algo_name} with {vec_name}: {e}\")\n",
    "                        mlflow.log_param(\"error\", str(e))\n",
    "\n",
    "def log_model_params(algo_name, model):\n",
    "    \"\"\"Logs hyperparameters of the trained model to MLflow.\"\"\"\n",
    "    params_to_log = {}\n",
    "    if algo_name == 'LogisticRegression':\n",
    "        params_to_log[\"C\"] = model.C\n",
    "    elif algo_name == 'MultinomialNB':\n",
    "        params_to_log[\"alpha\"] = model.alpha\n",
    "    elif algo_name == 'XGBoost':\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"learning_rate\"] = model.learning_rate\n",
    "    elif algo_name == 'RandomForest':\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"max_depth\"] = model.max_depth\n",
    "    elif algo_name == 'GradientBoosting':\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"learning_rate\"] = model.learning_rate\n",
    "        params_to_log[\"max_depth\"] = model.max_depth\n",
    "\n",
    "    mlflow.log_params(params_to_log)\n",
    "\n",
    "# ========================== EXECUTION ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data(CONFIG[\"data_path\"])\n",
    "    train_and_evaluate(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
